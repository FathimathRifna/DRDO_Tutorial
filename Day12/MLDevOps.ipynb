{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "270d95d704db4be78685e679789a465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_959d2993167d46c780925bb9fbf5014a",
              "IPY_MODEL_4382103e42ba499281047db3c61a6892",
              "IPY_MODEL_72e17dca628c47e893ae6c44d552b18a"
            ],
            "layout": "IPY_MODEL_ae0eb4dc19544659b39d6e0b82598099"
          }
        },
        "959d2993167d46c780925bb9fbf5014a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13324edc749241c4991148f786411f7b",
            "placeholder": "​",
            "style": "IPY_MODEL_cfac88f661574a768cdff6c29bc9612e",
            "value": "100%"
          }
        },
        "4382103e42ba499281047db3c61a6892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8651c1f51d664423982ac42974e51b3f",
            "max": 32342954,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e68f089628d5419cba9239ae6807ac3d",
            "value": 32342954
          }
        },
        "72e17dca628c47e893ae6c44d552b18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb771389ef343678333fc5235f58465",
            "placeholder": "​",
            "style": "IPY_MODEL_79320ae398c64d74beb6bcd8fdb805f8",
            "value": " 30.8M/30.8M [00:02&lt;00:00, 21.7MB/s]"
          }
        },
        "ae0eb4dc19544659b39d6e0b82598099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13324edc749241c4991148f786411f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfac88f661574a768cdff6c29bc9612e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8651c1f51d664423982ac42974e51b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68f089628d5419cba9239ae6807ac3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ceb771389ef343678333fc5235f58465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79320ae398c64d74beb6bcd8fdb805f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLDevOps - Deployment, Adaptation and Maintenance"
      ],
      "metadata": {
        "id": "tQMg_gjzIvQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is DevOps?\n",
        "\n",
        "![](https://ml-devops-tutorial.readthedocs.io/en/latest/_images/devops1.jpg)\n",
        "\n",
        "* it is not just tools\n",
        "* it is not just processes\n",
        "* it is not a trendy job title\n",
        "* it is not just automation\n",
        "\n",
        "DevOps is the practice of operations and development engineers participating together in the entire service lifecycle, from design through the development process to production support.\n",
        "\n",
        "![](https://ml-devops-tutorial.readthedocs.io/en/latest/_images/devops-whatisdevops.png)\n",
        "\n",
        "Adopting these practices and operations can lead to more robutst and reliable systems. As well as positively impact the whole development cycle: from R&D to development and production. Meaning: reducing deployment turnarounds, enhanced system montoring and alerting and better development planning. DevOps focuses on continuous integration and continuous delivery of software by leveraging on-demand IT resources (infrastructure as code) and by automating integration, test and deployment of code."
      ],
      "metadata": {
        "id": "04JvL6E_LU9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some concepts part of MLDevOps framework:"
      ],
      "metadata": {
        "id": "zDt9pXAsMC6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Development platform:** a collaborative platform for performing ML experiments and empowering the creation of ML models by data scientists should be considered part of the MLOps framework. This platform should enable secure access to data sources (e.g., from data engineering workflows). We want the handover from ML training to deployment to be as smooth as possible, which is more likely the case for such a platform than ML models developed in different local environments."
      ],
      "metadata": {
        "id": "R_ur3IhvM1uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Model unit testing:** every time we create, change, or retrain a model, we should automatically validate the integrity of the model, e.g.\n",
        "- should meet minimum performance on a test set\n",
        "- should perform well on synthetic use case-specific datasets"
      ],
      "metadata": {
        "id": "EudNhvguND3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Versioning:** it should be possible to go back in time to inspect everything relating to a given model, e.g., what data & code was used. Why? Because if something breaks, we need to be able to go back in time and see why."
      ],
      "metadata": {
        "id": "GmNazsV7NjcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Model registry:** there should be an overview of deployed & decommissioned ML models, their version history, and the deployment stage of each version. Why? If something breaks, we can roll back a previously archived version back into production."
      ],
      "metadata": {
        "id": "lKab1rLyNpCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Model Governance:** only certain people should have access to see training related to any given model. There should be access control for who can request/reject/approve transitions between deployment stages (e.g., dev to test to prod) in the model registry."
      ],
      "metadata": {
        "id": "DY1JowINNv1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Deployments:** deployment can be many things, but in this post, I consider the case where we want to deploy a model to cloud infrastructure and expose an API, which enables other people to consume and use the model, i.e., I’m not considering cases where we want to deploy ML models into embedded systems. Efficient model deployments on appropriate infrastructure should:\n",
        "- support multiple ML frameworks + custom models\n",
        "- have well-defined API spec (e.g., Swagger/OpenAPI)\n",
        "- support containerized model servers"
      ],
      "metadata": {
        "id": "SU8YipevNwXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Monitoring:** tracking performance metrics (throughput, uptime, etc.). Why? If suddenly a model starts returning errors or being unexpectedly slow, we need to know before the end-user complains so that we can fix it."
      ],
      "metadata": {
        "id": "Nwr-cvNmNzKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Feedback:** we need to feedback information to the model on how well it is performing. Why? Typically we run predictions on new samples where we do not yet know the ground truth. As we learn the truth, however, we need to inform the model to report on how well it is actually doing."
      ],
      "metadata": {
        "id": "7rr5IRalN1hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. A/B testing:** no matter how solid cross-validation we think we’re doing, we never know how the model will perform until it actually gets deployed. It should be easy to perform A/B experiments with live models within the MLOps framework."
      ],
      "metadata": {
        "id": "zzkliro5N5Tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Drift detection:** typically, the longer time a given model is deployed, the worse it becomes as circumstances change compared to when the model was trained. We can try to monitor and alert on these different circumstances, or “drifts” before they get too severe:\n",
        "- Concept drift: when the relation between input and output has changed\n",
        "- Prediction drift: changes in predictions, but the model still holds\n",
        "- Label drift: change in the model’s outcomes compared to training data\n",
        "- Feature drift: change in the distribution of model input data"
      ],
      "metadata": {
        "id": "DLvbOEWSN7gS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Outlier detection:** if a deployed model receives an input sample that is significantly different from anything observed during training, we can try to identify this sample as a potential outlier, and the returned prediction should be marked as such, indicating that the end-user should be careful in trusting the prediction."
      ],
      "metadata": {
        "id": "6lTbza7dN9Xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Adversarial Attack Detection:** we should be warned when adversarial samples attack our models (e.g., someone trying to abuse/manipulate the outcome of our algorithms)."
      ],
      "metadata": {
        "id": "93EEJsL9N_cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Interpretability:** the ML deployments should support endpoints returning the explanation of our prediction, e.g., through SHAP values. Why? for a lot of use cases, a prediction is not enough, and the end-user needs to know why a given prediction was made."
      ],
      "metadata": {
        "id": "RbAC2trYOA9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Governance of deployments:** we not only need access restrictions on who can see the data, trained models, etc., but also on who can eventually use the deployed models. These deployed models can often be just as confidential as the data they were trained on."
      ],
      "metadata": {
        "id": "fZJt1wUmOD-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Data-centricity:** rather than focus on model performance & improvements, it makes sense that an MLOps framework also enables an increased focus on how the end-user can improve data quality and breadth."
      ],
      "metadata": {
        "id": "LRBozKQxNLVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i9W68wpShSJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's deploy a simple classification model using Flask**"
      ],
      "metadata": {
        "id": "l5o5fHNEsutv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "dO4Ly9BMs7At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from flask import Flask, jsonify, request"
      ],
      "metadata": {
        "id": "Wux1Yw-ZhSrt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/serve/master/examples/image_classifier/index_to_name.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aU7BHIRnnW8",
        "outputId": "5af84e3f-2d59-4432-dfc8-5ed7fbd8d203"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-23 18:13:27--  https://raw.githubusercontent.com/pytorch/serve/master/examples/image_classifier/index_to_name.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35363 (35K) [text/plain]\n",
            "Saving to: ‘index_to_name.json’\n",
            "\n",
            "index_to_name.json  100%[===================>]  34.53K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-11-23 18:13:28 (8.32 MB/s) - ‘index_to_name.json’ saved [35363/35363]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to setup model for inference"
      ],
      "metadata": {
        "id": "7s6MWlsXtLoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "model = models.densenet121(pretrained=True)               # Trained on 1000 classes from ImageNet\n",
        "model.eval()                                              # Turns off autograd\n",
        "\n",
        "img_class_map = None\n",
        "mapping_file_path = 'index_to_name.json'                  # Human-readable names for Imagenet classes\n",
        "if os.path.isfile(mapping_file_path):\n",
        "    with open (mapping_file_path) as f:\n",
        "        img_class_map = json.load(f)\n",
        "\n",
        "# Transform input into the form our model expects\n",
        "def transform_image(infile):\n",
        "    input_transforms = [transforms.Resize(255),           # We use multiple TorchVision transforms to ready the image\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],       # Standard normalization for ImageNet model input\n",
        "            [0.229, 0.224, 0.225])]\n",
        "    my_transforms = transforms.Compose(input_transforms)\n",
        "    image = Image.open(infile)                            # Open the image file\n",
        "    timg = my_transforms(image)                           # Transform PIL image to appropriately-shaped PyTorch tensor\n",
        "    timg.unsqueeze_(0)                                    # PyTorch models expect batched input; create a batch of 1\n",
        "    return timg\n",
        "\n",
        "# Get a prediction\n",
        "def get_prediction(input_tensor):\n",
        "    outputs = model.forward(input_tensor)                 # Get likelihoods for all ImageNet classes\n",
        "    _, y_hat = outputs.max(1)                             # Extract the most likely class\n",
        "    prediction = y_hat.item()                             # Extract the int value from the PyTorch tensor\n",
        "    return prediction\n",
        "\n",
        "# Make the prediction human-readable\n",
        "def render_prediction(prediction_idx):\n",
        "    stridx = str(prediction_idx)\n",
        "    class_name = 'Unknown'\n",
        "    if img_class_map is not None:\n",
        "        if stridx in img_class_map is not None:\n",
        "            class_name = img_class_map[stridx][1]\n",
        "    return prediction_idx, class_name\n",
        "\n",
        "# Retrain model\n",
        "def retrain_network(file):\n",
        "  # Write code to retrain network here\n",
        "\n",
        "  return jsonify({'message': \"This is a new image!! Network has now been trained on this sample...\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "270d95d704db4be78685e679789a465d",
            "959d2993167d46c780925bb9fbf5014a",
            "4382103e42ba499281047db3c61a6892",
            "72e17dca628c47e893ae6c44d552b18a",
            "ae0eb4dc19544659b39d6e0b82598099",
            "13324edc749241c4991148f786411f7b",
            "cfac88f661574a768cdff6c29bc9612e",
            "8651c1f51d664423982ac42974e51b3f",
            "e68f089628d5419cba9239ae6807ac3d",
            "ceb771389ef343678333fc5235f58465",
            "79320ae398c64d74beb6bcd8fdb805f8"
          ]
        },
        "id": "zKVBErtxi-om",
        "outputId": "41292873-861f-4e0d-f11b-e1f45a14ccf0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/30.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "270d95d704db4be78685e679789a465d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define flask routes where the model API will be served"
      ],
      "metadata": {
        "id": "7yGtM4GztRiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/', methods=['GET'])\n",
        "def root():\n",
        "    return jsonify({'msg' : 'Try POSTing to the /predict endpoint with an RGB image attachment'})\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        if file is not None:\n",
        "            input_tensor = transform_image(file)\n",
        "            prediction_idx = get_prediction(input_tensor)\n",
        "            class_id, class_name = render_prediction(prediction_idx)\n",
        "            if class_name == \"Unknown\":\n",
        "              return retrain_network(file)\n",
        "            return jsonify({'class_id': class_id, 'class_name': class_name})"
      ],
      "metadata": {
        "id": "QNDDGguZjCbo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Flask server in background"
      ],
      "metadata": {
        "id": "UtByHRhhtcCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def network_call():\n",
        "  app.run()\n",
        "threading.Thread(target=network_call).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r04TXrKdjP2I",
        "outputId": "23f7bd84-9a34-4ced-ebd4-2b14e713bff6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on a cat image"
      ],
      "metadata": {
        "id": "cBcDL61YtfDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/serve/master/examples/image_classifier/kitten.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5OhTx0SnbXQ",
        "outputId": "31b8244b-8f1d-4f16-b08e-b3b43401d049"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-23 18:13:42--  https://raw.githubusercontent.com/pytorch/serve/master/examples/image_classifier/kitten.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110969 (108K) [image/jpeg]\n",
            "Saving to: ‘kitten.jpg’\n",
            "\n",
            "kitten.jpg          100%[===================>] 108.37K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-11-23 18:13:42 (4.11 MB/s) - ‘kitten.jpg’ saved [110969/110969]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -X POST -H \"Content-Type: multipart/form-data\" http://localhost:5000/predict -F \"file=@kitten.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUr75yx4jWDg",
        "outputId": "ebdb52e9-c5f6-4db7-9b12-d9635476dc78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Nov/2022 18:11:46] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"class_id\":282,\"class_name\":\"tiger_cat\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrain network when unknown data is uploaded"
      ],
      "metadata": {
        "id": "dtKQlMyltiJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -X POST -H \"Content-Type: multipart/form-data\" http://localhost:5000/predict -F \"file=@unknown.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBFmZ3T5sHFN",
        "outputId": "6d078747-3256-4ebe-e166-309fdb195b1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Nov/2022 18:13:47] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"message\":\"This is a new image!! Network has now been trained on this sample...\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASS ASSIGNMENT:**\n",
        "\n",
        "**Above we used the Imagenet1000 pretrained DensetNet121 model. Now do the same, but for the model that we had trained in an earlier tutorial. The model was used for predicting the digit present in an image (MNIST Dataset). Additionally log and plot the classification IDs inferenced so far by the model using a tool like [WandB](https://wandb.ai/).**"
      ],
      "metadata": {
        "id": "r-iZ01gltqK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code here"
      ],
      "metadata": {
        "id": "Xx2gBxOOuLIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}