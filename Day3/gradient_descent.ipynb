{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215d9cd7",
   "metadata": {},
   "source": [
    "# Gradient Descent Tutorial\n",
    "\n",
    "Link to Presentation: https://docs.google.com/presentation/d/1basYOmB3uW8l-p-v1OChzzYI0bzm-r3dr8wFTRpluhk/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a1283",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a5d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dffef2",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "Definitions of various functions and their gradients. We will try to reach the minimas of these functions starting from a random initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bb8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    return np.sum(np.square(X))\n",
    "def grad_f1(X):\n",
    "    grad = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        grad[i] = 2*X[i]\n",
    "    return grad\n",
    "\n",
    "def f2(X):\n",
    "    my_sum=0\n",
    "    for i in range(len(X)):\n",
    "        my_sum+= math.sin(X[i])\n",
    "    return my_sum\n",
    "def grad_f2(X):\n",
    "    epsilon = 1e-6\n",
    "    grad = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        grad[i] = math.cos(X[i])\n",
    "    return grad\n",
    "\n",
    "def f3(X):\n",
    "    return math.sin(np.sum(X))\n",
    "def grad_f3(X):\n",
    "    return math.cos(np.sum(X))\n",
    "\n",
    "def f4(X):\n",
    "    my_sum = 0\n",
    "    for i in range(len(X)):\n",
    "        my_sum+= (i+1) * (X[i] ** (i+1))\n",
    "    return my_sum\n",
    "\n",
    "def grad_f4(X):\n",
    "    grad = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "#         print(i,X[i])\n",
    "        grad[i] = (i+1) * ((i+1) * (X[i] ** i))\n",
    "    return grad\n",
    "\n",
    "def f5(X):\n",
    "    coeffs = np.zeros_like(X)\n",
    "    my_vars = np.zeros_like(X)\n",
    "    for i in range(len(coeffs)):\n",
    "        coeffs[i] = (i+1)\n",
    "        my_vars[i] = (X[i] ** (i+1))\n",
    "    return coeffs.T @ my_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80cf9d9",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3366e0",
   "metadata": {},
   "source": [
    "**Note:** Talk about convergence_threshold\n",
    "- Just a bargain between time and precision\n",
    "- Near the minima, update rate becomes very slow\n",
    "- Also, getting an exact match for complex functions is difficult (splly when going stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb6f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_v1(X_init, f, grad_f, convergence_threshold=1e-6, max_iters=1000):\n",
    "    X_old = X_init # Initialize X_old with the initial value chose for X\n",
    "    iter_count = 0\n",
    "    while iter_count < max_iters:\n",
    "        iter_count+= 1\n",
    "        output_old = f(X_old)\n",
    "        grad = grad_f(X_old)\n",
    "        X_new = X_old - grad\n",
    "        output_new = f(X_new)\n",
    "        X_old = X_new # Once X has been updated, now the new X becomes the old X\n",
    "        if abs(output_new - output_old) < convergence_threshold:\n",
    "            break\n",
    "    return X_old, iter_count\n",
    "\n",
    "def grad_desc_v2(X_init, f, grad_f, convergence_threshold=1e-6, max_iters=1000):\n",
    "    \"\"\" This function is same as the v1 function except that we print the intermediate values which X takes \"\"\"\n",
    "    \n",
    "    print(\"##### Function 2 log #####\")\n",
    "    X_old = X_init # Initialize X_old with the initial value chose for X\n",
    "    iter_count = 0\n",
    "    while iter_count < max_iters:\n",
    "        iter_count+= 1\n",
    "        output_old = f(X_old)\n",
    "        grad = grad_f(X_old)\n",
    "        X_new = X_old - grad\n",
    "        output_new = f(X_new)\n",
    "        print(\"Prev X, New X, grad\", X_old, X_new, grad)\n",
    "        X_old = X_new # Once X has been updated, now the new X becomes the old X\n",
    "        if abs(output_new - output_old) < convergence_threshold:\n",
    "            break\n",
    "    print(\"##### GD Version 2 Log End #####\")\n",
    "    return X_old, iter_count\n",
    "\n",
    "def grad_desc_v3(X_init, f, grad_f, convergence_threshold=1e-6, max_iters=1000):\n",
    "    \"\"\" This function is same as the v1 function except that we have remove convergence threshold criteria\n",
    "    and are printing intermediate values \"\"\"\n",
    "\n",
    "    print(\"##### GD Version 3 log #####\")\n",
    "    X_old = X_init # Initialize X_old with the initial value chose for X\n",
    "    iter_count = 0\n",
    "    while iter_count < max_iters:\n",
    "        iter_count+= 1\n",
    "        output_old = f(X_old)\n",
    "        grad = grad_f(X_old)\n",
    "        X_new = X_old - grad\n",
    "        output_new = f(X_new)\n",
    "        print(\"Prev X, New X, grad\", X_old, X_new, grad)\n",
    "        X_old = X_new # Once X has been updated, now the new X becomes the old X\n",
    "#         if abs(output_new - output_old) < convergence_threshold:\n",
    "#             break\n",
    "    print(\"##### Function 3 Log End #####\")\n",
    "    return X_old, iter_count\n",
    "\n",
    "def grad_desc_v4(X_init, f, grad_f, convergence_threshold=1e-6, max_iters=1000, lr=1e-3):\n",
    "    \"\"\" This function is same as grad_desc_v1() except that it incorporates learning rate too\"\"\"\n",
    "    X_old = X_init # Initialize X_old with the initial value chose for X\n",
    "    iter_count = 0\n",
    "    while iter_count < max_iters:\n",
    "        iter_count+= 1\n",
    "        output_old = f(X_old)\n",
    "        grad = grad_f(X_old)\n",
    "        X_new = X_old - lr*grad\n",
    "        output_new = f(X_new)\n",
    "        X_old = X_new # Once X has been updated, now the new X becomes the old X\n",
    "        if abs(output_new - output_old) < convergence_threshold:\n",
    "            break\n",
    "    return X_old, iter_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3a8ac",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2fed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### ##### ##### #####\n",
      "##### Post Function #####\n",
      "X_init [8.98896285]\n",
      "X_min [-8.98896285]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 80.80145316068914\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v1(X_init, f1, grad_f1, max_iters=100)\n",
    "print(\"##### ##### ##### #####\")\n",
    "print(\"##### Post Function #####\")\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12279229",
   "metadata": {},
   "source": [
    "**Question:** We don't seem to be converging towards the minima? Why is that the case? Let's see the intermediate values of the function (call grad_desc_v2() to see the intermediate values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001fb5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Function 2 log #####\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "##### GD Version 2 Log End #####\n",
      "##### ##### ##### #####\n",
      "##### Post Function #####\n",
      "X_init [8.98896285]\n",
      "X_min [-8.98896285]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 80.80145316068914\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v2(X_init, f1, grad_f1, max_iters=100)\n",
    "print(\"##### ##### ##### #####\")\n",
    "print(\"##### Post Function #####\")\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648993b",
   "metadata": {},
   "source": [
    "**Question:** Why does the algo stop after just 1 iteration? Let's fix this in grad_desc_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32457209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### GD Version 3 log #####\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "Prev X, New X, grad [8.98896285] [-8.98896285] [17.9779257]\n",
      "Prev X, New X, grad [-8.98896285] [8.98896285] [-17.9779257]\n",
      "##### Function 3 Log End #####\n",
      "##### ##### ##### #####\n",
      "##### Post Function #####\n",
      "X_init [8.98896285]\n",
      "X_min [8.98896285]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 80.80145316068914\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v3(X_init, f1, grad_f1, max_iters=100)\n",
    "print(\"##### ##### ##### #####\")\n",
    "print(\"##### Post Function #####\")\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eab9ca",
   "metadata": {},
   "source": [
    "**Observation:** The function seems to be oscillating between two values. Why is that the case?\n",
    "**Answer:** Because we have a large step size (see presentation for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b24e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [8.98896285]\n",
      "X_min [1.19211226]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 1.4211316438549106\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f1, grad_f1, max_iters=100, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab94ef",
   "metadata": {},
   "source": [
    "**Question:** Can we get even closer to the minima? How about continuing for more iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfede6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [8.98896285]\n",
      "X_min [0.00489564]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 2.3967320670757484e-05\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f1, grad_f1, max_iters=1000, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf321362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [8.98896285]\n",
      "X_min [0.00489564]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 2.3967320670757484e-05\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f1, grad_f1, max_iters=10000, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf481f",
   "metadata": {},
   "source": [
    "**Question:** Increasing iterations from 100 too 1000 helped. But increasing from 1000 to 10000 didn't. But can we still get even closer to the minima? How about decreasing the convergence threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34100001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [8.98896285]\n",
      "X_min [0.00048931]\n",
      "f1(X_init) 80.80145316068914\n",
      "f1(X_min) 2.3942238045035706e-07\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(10, 100, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f1, grad_f1, max_iters=10000, convergence_threshold=1e-8, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100e39e",
   "metadata": {},
   "source": [
    "### What about other functions and other inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6236aa",
   "metadata": {},
   "source": [
    "#### F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b06a8",
   "metadata": {},
   "source": [
    "##### (a) 1 input, different range (including negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd743bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [1.49816048]\n",
      "X_min [0.00049239]\n",
      "f1(X_init) 2.244484810019143\n",
      "f1(X_min) 2.4244920836708565e-07\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 1\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f1, grad_f1, max_iters=10000, convergence_threshold=1e-8, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0517d",
   "metadata": {},
   "source": [
    "##### (b) 10 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b867b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-3.37086107  0.95071431 -3.65996971 -5.38792636 -1.5601864   0.15599452\n",
      "  0.05808361  5.19705687 -0.60111501  3.54036289]\n",
      "X_min [-1.65860900e-04  4.67792435e-05 -1.80086292e-04 -2.65109210e-04\n",
      " -7.67678989e-05  7.67560307e-06  2.85796418e-06  2.55717609e-04\n",
      " -2.95774507e-05  1.74201121e-04]\n",
      "f1(X_init) 97.05848916542024\n",
      "f1(X_min) 2.3498484648264253e-07\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 10\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f1, grad_f1, max_iters=10000, convergence_threshold=1e-8, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f1(X_init)\", f1(X_init))\n",
    "print(\"f1(X_min)\", f1(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6e00a",
   "metadata": {},
   "source": [
    "#### F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1fe65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-3.37086107  0.95071431 -3.65996971 -5.38792636 -1.5601864   0.15599452\n",
      "  0.05808361  5.19705687 -0.60111501  3.54036289]\n",
      "X_min [-1.57104064 -1.57018686 -1.57113081 -7.85342617 -1.5707953  -1.57056969\n",
      " -1.57059107  4.71243669 -1.57069457  4.71226067]\n",
      "f2(X_init) -0.3082695731102191\n",
      "f2(X_min) -9.999999512931305\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 10\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f2, grad_f2, max_iters=10000, convergence_threshold=1e-8, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f2(X_init)\", f2(X_init))\n",
    "print(\"f2(X_min)\", f2(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bc104",
   "metadata": {},
   "source": [
    "#### F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db219386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-3.37086107  0.95071431 -3.65996971 -5.38792636 -1.5601864   0.15599452\n",
      "  0.05808361  5.19705687 -0.60111501  3.54036289]\n",
      "X_min [-3.0601836   1.26139178 -3.34929224 -5.07724889 -1.24950894  0.46667199\n",
      "  0.36876108  5.50773434 -0.29043754  3.85104036]\n",
      "f3(X_init) 0.9994034626647199\n",
      "f3(X_min) -0.999999962094758\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 10\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f3, grad_f3, max_iters=10000, convergence_threshold=1e-8, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f3(X_init)\", f3(X_init))\n",
    "print(\"f3(X_min)\", f3(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c761a",
   "metadata": {},
   "source": [
    "#### F4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63586eef",
   "metadata": {},
   "source": [
    "##### (a) With 10 vars -> Needs low LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eabd326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-3.37086107  0.95071431 -3.65996971 -5.38792636 -1.5601864   0.15599452\n",
      "  0.05808361  5.19705687 -0.60111501  3.54036289]\n",
      "X_min [-1.03370861e+002  4.90208034e-178             -inf              nan\n",
      "             -inf  8.87883128e-002  5.78972632e-002              nan\n",
      "             -inf              nan]\n",
      "f4(X_init) 7354324.9358452065\n",
      "f4(X_min) nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40438/1343806137.py:29: RuntimeWarning: overflow encountered in double_scalars\n",
      "  my_sum+= (i+1) * (X[i] ** (i+1))\n",
      "/tmp/ipykernel_40438/1343806137.py:36: RuntimeWarning: overflow encountered in double_scalars\n",
      "  grad[i] = (i+1) * ((i+1) * (X[i] ** i))\n",
      "/tmp/ipykernel_40438/3105118052.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if abs(output_new - output_old) < convergence_threshold:\n",
      "/tmp/ipykernel_40438/3105118052.py:62: RuntimeWarning: invalid value encountered in subtract\n",
      "  X_new = X_old - lr*grad\n",
      "/tmp/ipykernel_40438/1343806137.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  my_sum+= (i+1) * (X[i] ** (i+1))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 10\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-2)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927a2e4",
   "metadata": {},
   "source": [
    "**Note:** If you read through the error message, you'll read about encountering overflows, and you can see nan and inf values in the outputs too. This is a sign that our algo is diverging. To fix this, we should try using a lower learning rate.\n",
    "\n",
    "Another \"tricky\" way to deal with this can be to reduce the ```max_iters``` count. However, even though that might work in some rare cases, it doesn't solve the root of the problem and is a bad way to solve this problem. Instead, we should use this information to doubly check our algorithms (**How**)?\n",
    "\n",
    "-> Increase max_iters and see what happens? (What is expected?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d9d396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-3.37086107  0.95071431 -3.65996971 -5.38792636 -1.5601864   0.15599452\n",
      "  0.05808361  5.19705687 -0.60111501  3.54036289]\n",
      "X_min [-3.37096107  0.9503341  -3.67206539 -5.15385586 -1.57528714  0.15599419\n",
      "  0.05808361  1.72115951 -0.60125322  1.37108025]\n",
      "f4(X_init) 7354324.9358452065\n",
      "f4(X_min) 3474.378676869462\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 10\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-8)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d286c",
   "metadata": {},
   "source": [
    "**Observation:** Notice how we need to keep the learning rate pretty small here\n",
    "The reason for doing this is that exponents of 10 would give really large values, so to keep the step size small enough, we'll need a very small learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bb873",
   "metadata": {},
   "source": [
    "##### (b) With 3 vars -> Needs HIGHER LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "652b846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-1.49816048  7.60571445  0.        ]\n",
      "X_min [-1.49826048  7.60267277  0.        ]\n",
      "f4(X_init) 114.195624153409\n",
      "f4(X_min) 114.10300613599755\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 3\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-8)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618dbca",
   "metadata": {},
   "source": [
    "**Observation:** Here, moving ahead with such a low lr, leads to very slow convergence (see that the value is changing, but very slowly). This is because such a low lr makes the step size extremely small here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71726636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-1.49816048  7.60571445  0.        ]\n",
      "X_min [-1.49916048  7.57535236  0.        ]\n",
      "f4(X_init) 114.195624153409\n",
      "f4(X_min) 113.27276620630853\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 3\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=100000, convergence_threshold=1e-8, lr=1e-8)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b35479",
   "metadata": {},
   "source": [
    "**Note:** Increasing number of iterations takes us closer to minima, but still the speed of convergence is very slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ac62590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-1.49816048  7.60571445  0.        ]\n",
      "X_min [-1.50116048  7.5149913   0.        ]\n",
      "f4(X_init) 114.195624153409\n",
      "f4(X_min) 111.44902811131924\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 3\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=300000, convergence_threshold=1e-8, lr=1e-8)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8adb44",
   "metadata": {},
   "source": [
    "**Note:** Increasing number of iterations reduces the function further. But still we are very far away from the minima due to a very small step size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff5ed778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-1.49816048  7.60571445  0.        ]\n",
      "X_min [-1.14981605e+01  2.98211408e-17  0.00000000e+00]\n",
      "f4(X_init) 114.195624153409\n",
      "f4(X_min) -11.498160475388572\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 3\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-3)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0d826",
   "metadata": {},
   "source": [
    "**Note:** For polynomial of degree 3 (note that degree of polynomial = number of variables in it, only for this particular function {f4()} because we have defined it that way), using a learning rate of 1e-3 seems to work fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916b2ad",
   "metadata": {},
   "source": [
    "##### (c) With 5 variables -> Needs \"mid\" lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0631263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40438/1343806137.py:29: RuntimeWarning: overflow encountered in double_scalars\n",
      "  my_sum+= (i+1) * (X[i] ** (i+1))\n",
      "/tmp/ipykernel_40438/1343806137.py:36: RuntimeWarning: overflow encountered in double_scalars\n",
      "  grad[i] = (i+1) * ((i+1) * (X[i] ** i))\n",
      "/tmp/ipykernel_40438/3105118052.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if abs(output_new - output_old) < convergence_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [ 2.99632095  0.          0.         -4.19060939 -0.46805592]\n",
      "X_min [-7.00367905  0.          0.         -0.05587787        -inf]\n",
      "f4(X_init) 1236.4679733530675\n",
      "f4(X_min) -inf\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 5\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-3)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30df55",
   "metadata": {},
   "source": [
    "**Note:** Shows diverging tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2d51ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [ 2.99632095  0.          0.         -4.19060939 -0.46805592]\n",
      "X_min [ 2.99622095  0.          0.         -4.07760266 -0.46817597]\n",
      "f4(X_init) 1236.4679733530675\n",
      "f4(X_min) 1108.691444250409\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 5\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-8)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418eb6e",
   "metadata": {},
   "source": [
    "**Note:** Slow steps towards convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aceb041",
   "metadata": {},
   "source": [
    "**Note:** For degree 10, we used lr=1e-8, for degree 3, we used lr=1e-3. But for 5, we need to find something in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61a742bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-1.49816048  7.60571445  0.        ]\n",
      "X_min [-1.59816048  5.09822207  0.        ]\n",
      "f4(X_init) 114.195624153409\n",
      "f4(X_min) 50.38557615804782\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 3\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f4, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-5)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f4(X_init)\", f4(X_init))\n",
    "print(\"f4(X_min)\", f4(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0f834",
   "metadata": {},
   "source": [
    "#### F5 (Using grad of f4 but definition of f5)\n",
    "- This just shows an alternate way to represent a function involving addition. Will discuss about this representation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63e802c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init [-1.49816048  7.60571445  0.        ]\n",
      "X_min [-1.59816048  5.09822207  0.        ]\n",
      "f5(X_init) 114.195624153409\n",
      "f5(X_min) 50.38557615804782\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_vars = 3\n",
    "X_init = np.random.random((num_vars,)) * np.random.randint(-10, 10, num_vars)\n",
    "X_min, iter_count = grad_desc_v4(X_init, f5, grad_f4, max_iters=10000, convergence_threshold=1e-8, lr=1e-5)\n",
    "print(\"X_init\", X_init)\n",
    "print(\"X_min\", X_min)\n",
    "print(\"f5(X_init)\", f5(X_init))\n",
    "print(\"f5(X_min)\", f5(X_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1357ff3",
   "metadata": {},
   "source": [
    "##### Moral\n",
    "Choice of learning rate is a hyperparameter which needs to be chosen carefully based on the values in the data and some experimentation (Comment: Weighing of losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb4ccf",
   "metadata": {},
   "source": [
    "#### Try for more functions\n",
    "All you need to do is:\n",
    "1. Define your own function which takes an input and returns an output\n",
    "2. Define a function which returns the gradient of that function\n",
    "3. Pass these to our gradient descent function\n",
    "4. Tune the hyper-parameters (like max_iters, lr etc. Particularly lr)\n",
    "\n",
    "**Note**: You need not choose powers of 10 only! You can try different values (like 3e-3) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5586988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:experimental]",
   "language": "python",
   "name": "conda-env-experimental-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
